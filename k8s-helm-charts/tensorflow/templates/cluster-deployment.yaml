{{- $relname := .Release.Namespace -}}
{{- $storname := .Values.storage.name -}}
{{- $repo := .Values.tfCluster.image.repo -}}
{{- $image := .Values.tfCluster.image.name -}}
{{- $tagGpu := .Values.tfCluster.image.dockerTagGpu -}}
{{- $tagCpu := .Values.tfCluster.image.dockerTagCpu -}}
{{- $port := .Values.tfCluster.service.internalPort -}}
{{- $nbGpu := .Values.tfCluster.settings.nbGpuPerNode -}}
{{- $isGpu := .Values.tfCluster.settings.isGpu -}}
{{- $dataset := .Values.tfCluster.settings.dataset -}}
{{- $resources := .Values.tfCluster.resources -}}
{{- $mode := .Values.tfCluster.settings.mode -}}

---
# Defining a generic configuration file for the cluster
apiVersion: v1
kind: ConfigMap
metadata:
  name: tensorflow-cluster-config
  namespace: {{.Release.Namespace}}
data:
  clusterconfig: >
        {
        {{- range $job, $nb := .Values.tfCluster.settings.jobs }}
          {{ $job | quote }}: [
          {{ range $i, $e := until (int $nb) | default 8 }}
            "{{ $job }}-{{$i}}.{{ $relname }}.svc.cluster.local:8080",
          {{ end }}
              ],
        {{- end }}
        }
---
{{- range $job, $nb := .Values.tfCluster.settings.jobs }}
{{ range $i, $e := until (int $nb) | default 8 }}
# Definiting a scalable cluster
{{ if eq $mode "continuous" }}
apiVersion: extensions/v1beta1
kind: Deployment
{{ else }}
apiVersion: batch/v1
kind: Job
{{ end }}
metadata:
  name: {{ $job }}-{{$i}}
  namespace: {{ $relname }}
spec:
{{ if eq $mode "continuous" }}
  replicas: 1
  selector:
    matchLabels:
      job: {{ $job }}
      task: t{{$i}}
{{ else }}
  parallelism: 1
{{ end }}
  template:
    metadata:
      labels:
        job: {{ $job }}
        task: t{{$i}}
    spec:
      containers:
      - name: tf-grpc-server
        {{ if eq $job "worker" }}
        {{ if $isGpu }}
        image: {{ $repo }}/{{ $image }}:{{ $tagGpu }}
        resources:
          requests:
{{ toYaml $resources.requests | indent 12 }}
            alpha.kubernetes.io/nvidia-gpu: {{ $nbGpu }}
          limits:
{{ toYaml $resources.limits | indent 12 }}           
            alpha.kubernetes.io/nvidia-gpu: {{ $nbGpu }}
        {{ else }}
        image: {{ $repo }}/{{ $image }}:{{ $tagCpu }}        
        resources:
{{ toYaml $resources | indent 10 }}
        {{ end }}
        {{ else }}
        image: {{ $repo }}/{{ $image }}:{{ $tagCpu }}        
        resources:
{{ toYaml $resources | indent 10 }}
        {{ end }}
        command: [ "/{{ $job }}.sh" ]
        ports:
        - name: grpc-server
          containerPort: {{ $port }}
        env:
        - name: DATASET
          value: {{ $dataset }}
        - name: DATA_DIR
          value: /var/tensorflow/{{ $dataset }}
        - name: GRPC_VERBOSITY
          value: DEBUG
        - name: POD_NAME
          value: {{ $job }}-{{ $i }}
        - name: CLUSTER_CONFIG
          valueFrom:
            configMapKeyRef:
              name: tensorflow-cluster-config
              key: clusterconfig
        - name: LD_LIBRARY_PATH
          value: "$LD_LIBRARY_PATH:/usr/lib/nvidia:/usr/lib/cuda"
        volumeMounts:
        - name: {{ $storname }}
          mountPath: /var/tensorflow/{{ $dataset }}
        {{ if eq $job "worker" }}
        {{ if $isGpu }}
        - mountPath: /usr/local/nvidia/bin
          name: bin
        - mountPath: /usr/lib/nvidia
          name: lib
        - mountPath: /usr/lib/cuda
          name: libcuda
        {{ end }}
        {{ end }}
      volumes:
      - name: {{ $storname }}
        persistentVolumeClaim:
          claimName: {{ $storname }}
      {{ if eq $job "worker" }}
      {{ if $isGpu }}
      - name: bin
        hostPath: 
          path: /usr/lib/nvidia-384/bin
      - name: lib
        hostPath: 
          path: /usr/lib/nvidia-384
      - name: libcuda
        hostPath: 
          path: /usr/lib/x86_64-linux-gnu
      {{ end }}
      {{ end }}
---
{{ end }}
{{- end }}
